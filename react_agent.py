from typing import List, Dict
from agent_actions import search_context, call_web_search
from constant import GOOGLE_GEMINI_API_KEY, GOOGLE_GEMINI_MODEL_NAME

import google.generativeai as genai
import json
import re
import os
from datetime import datetime

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Google Gemini API
genai.configure(api_key=GOOGLE_GEMINI_API_KEY)
gemini_client = genai.GenerativeModel(GOOGLE_GEMINI_MODEL_NAME)


class ReActAgent:
    def __init__(self, max_steps: int = 5, enable_logging: bool = True):
        self.observations: List[str] = []
        self.max_steps = max_steps
        self.enable_logging = enable_logging
        self.log_lines: List[str] = []

    # -------------------------
    # Logging helpers
    # -------------------------
    def log(self, message: str):
        if self.enable_logging:
            self.log_lines.append(message)
            print(message)

    # -------------------------
    # Save log to file (output data/debug/<file>.md)
    # -------------------------
    def save_log(self):
        output_dir = "data/debug"
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filepath = f"{output_dir}/react_agent_log_{timestamp}.md"
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
        os.makedirs(output_dir, exist_ok=True)
        
        with open(filepath, "w", encoding="utf-8") as f:
            f.write("\n".join(self.log_lines))

    # -------------------------
    # Reasoning step
    # -------------------------
    def reason(self, user_input: str, observations: List[str], step: int) -> Dict[str, str]:
        """
        LLM reasoning - let AI decide what to do based on the query type and current observations
        """

        # -------------------------
        # Clean ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• observations ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö input ‡∏Ç‡∏≠‡∏á LLM
        # -------------------------
        # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏≠‡∏≤ HTML tags ‡∏≠‡∏≠‡∏Å‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏° "FINAL_STEP" 
        obs_texts = [re.sub(r"<[^>]+>", "", o) for o in observations if o != "FINAL_STEP"]
        # ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
        obs_text = " ".join(obs_texts)
        # ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏Å‡∏¥‡∏ô‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô
        obs_text = re.sub(r"\s+", " ", obs_text).strip()

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ LLM ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à
        prompt = f"""
        You are a ReAct Agent AI assistant. Analyze the current situation and decide the next action.
        
        Current observations: {obs_text if obs_text else "No observations yet"}
        User question: {user_input}
        Current step: {step}

        Available actions:
        - 'search_context' if you need to search the internal knowledge base/RAG system for relevant information
        - 'web_search' if you need additional current/external information from the internet
        - 'final_answer' if you have sufficient information to provide a complete answer

        Decision criteria:
        - If no observations yet, consider what type of information is needed first
        - If observations exist, evaluate if they provide sufficient information
        - Consider whether the question requires real-time/current information vs historical/factual information
        - Think about the most logical sequence of actions to answer the question effectively
        
        Respond in JSON format: 
        
        Query field should contain:
        - For 'search_context': search keywords or phrases for the knowledge base
        - For 'web_search': search query for the internet
        - For 'final_answer': the original user question being answered
        """
        try:
            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Gemini AI ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏ó‡∏≥‡∏ï‡πà‡∏≠‡πÑ‡∏õ
            response = gemini_client.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.2,  # ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏£‡∏Ñ‡πå‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á
                    max_output_tokens=500  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î token ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö
                )
            )

            # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å response
            text = getattr(response, "text", None)
            if not text:
                raise ValueError("No text returned from LLM")
            # ‡∏•‡∏ö code block markers ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å JSON response
            text = re.sub(r"^```json|```$", "", text, flags=re.MULTILINE).strip()
            # ‡πÅ‡∏õ‡∏•‡∏á JSON string ‡πÄ‡∏õ‡πá‡∏ô dictionary
            decision = json.loads(text)
        except Exception as e:
            # ‡∏´‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à default
            self.log(f"Error parsing LLM response: {e}")
            decision = {"action": "final_answer", "query": user_input}

        return decision

    # -------------------------
    # Action step
    # -------------------------
    def act(self, action_type: str, query: str, user_input: str = None) -> str:
        """
        Execute action and return observation
        """
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏≠‡∏á action_type ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥
        if action_type == "search_context":
            # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô (RAG system) ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
            docs = search_context(query, top_k=2) 
            if docs:
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
                doc_summaries = [f"Title: {doc['title']}, Content: {doc['content']}" for doc in docs]
                obs = f"Found {len(docs)} relevant document(s) in knowledge base: " + "; ".join(doc_summaries)
            else:
                # ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô (RAG system)
                obs = "No relevant information found in the internal knowledge base"
        elif action_type == "web_search":
            # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏≠‡∏¥‡∏ô‡πÄ‡∏ó‡∏≠‡∏£‡πå‡πÄ‡∏ô‡πá‡∏ï ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
            obs = call_web_search(query)
        elif action_type == "final_answer":
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡πÑ‡∏î‡πâ
            obs = self.generate_final_answer(user_input or query)
        else:
            # ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏ó‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å
            obs = f"Unknown action: {action_type}"

        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡∏•‡∏á‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ observations
        self.observations.append(obs)
        return obs

    # -------------------------
    # Final answer generation
    # -------------------------
    def generate_final_answer(self, user_input: str) -> str:
        """
        Generate final answer based on all current observations (true ReAct pattern)
        """
        # Clean ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• observations ‡πÇ‡∏î‡∏¢‡∏•‡∏ö HTML tags ‡∏≠‡∏≠‡∏Å
        obs_texts = [re.sub(r"<[^>]+>", "", o) for o in self.observations]
        # ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
        obs_text = " ".join(obs_texts)
        # ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏•‡∏∞ normalize ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        obs_text = re.sub(r"\s+", " ", obs_text).strip()

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å observations ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if not obs_text or obs_text.strip() == "":
            # ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å observations ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
            prompt = f"""
            Please provide a helpful, direct answer to this user question:

            {user_input}

            Provide a clear, informative response in 2-3 sentences.
            """
        else:
            # ‡∏Å‡∏£‡∏ì‡∏µ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å observations ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏±‡πâ‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö
            prompt = f"""
            Based on the information gathered, provide a clear final answer:

            Question: {user_input}
            Information gathered: {obs_text}

            Provide a complete answer in 2-3 clear sentences.
            """

        try:
            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Gemini AI ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
            response = gemini_client.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.1,  # ‡πÉ‡∏ä‡πâ temperature ‡∏ï‡πà‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
                    max_output_tokens=2000  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô token ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
                )
            )
            
            # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å response
            final_answer = getattr(response, "text", None)
            if not final_answer:
                raise ValueError("No text returned from LLM")
            
            # ‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ö prefix "FINAL_ANSWER: "
            return f"FINAL_ANSWER: {final_answer}"
                
        except Exception as e:
            # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å log
            self.log(f"Error generating final answer: {type(e).__name__}: {e}")
            return f"FINAL_ANSWER: Unable to generate final answer due to error: {e}"



    # -------------------------
    # Main agent flow
    # -------------------------
    def run(self, user_input: str) -> str:
        # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÉ‡∏´‡∏°‡πà‡πÇ‡∏î‡∏¢‡πÄ‡∏Ñ‡∏•‡∏µ‡∏¢‡∏£‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤
        self.observations = []
        self.log_lines = []
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á log header ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ô‡∏µ‡πâ
        self.log(f"# ReAct Agent Log")
        self.log(f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        self.log(f"**User Query:** {user_input}\n")
        self.log(f"## üöÄ Starting ReAct Agent Process\nMaximum Steps: {self.max_steps}\n")
        
        # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
        for step in range(1, self.max_steps + 1):
            # ‡πÉ‡∏´‡πâ LLM ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏ó‡∏≥‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ï‡∏≤‡∏°‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
            decision = self.reason(user_input, self.observations, step)
            action_type = decision.get("action", "final_answer")
            query = decision.get("query", user_input)
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å log ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ
            self.log(f"# Step {step}/{self.max_steps}")
            self.log(f"**Thought:** LLM decided to perform '{action_type}' action")
            self.log(f"**Action:** {action_type}")
            self.log(f"**Query:** {query}")

            # ‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà LLM ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ö‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï
            obs = self.act(action_type, query, user_input)
            self.log(f"**Observation:** {obs}\n")

            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏î‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if action_type == "final_answer":
                # ‡∏î‡∏∂‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï (‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö ReAct ‡πÅ‡∏ó‡πâ‡∏à‡∏£‡∏¥‡∏á)
                if obs.startswith("FINAL_ANSWER: "):
                    final_answer = obs[14:]  # ‡∏•‡∏ö prefix "FINAL_ANSWER: " ‡∏≠‡∏≠‡∏Å
                    self.log(f"=== Final Answer ===\n{final_answer}\n")
                    self.save_log()
                    return final_answer
                break

        # ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡∏à‡∏ö‡∏•‡∏π‡∏õ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ final_answer action (fallback)
        self.log("‚ö†Ô∏è  Agent reached maximum steps without final_answer action")
        self.log("üîÑ  Forcing final answer generation...\n")
        
        # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
        fallback_obs = self.generate_final_answer(user_input)
        if fallback_obs.startswith("FINAL_ANSWER: "):
            fallback_answer = fallback_obs[14:]  # ‡∏•‡∏ö prefix "FINAL_ANSWER: " ‡∏≠‡∏≠‡∏Å
            self.log(f"=== Fallback Answer ===\n{fallback_answer}\n")
            self.save_log()
            return fallback_answer
        else:
            # ‡∏Å‡∏£‡∏ì‡∏µ‡∏û‡∏¥‡πÄ‡∏®‡∏©: ‡πÅ‡∏°‡πâ‡πÅ‡∏ï‡πà generate_final_answer ‡∏Å‡πá‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß
            error_msg = f"Unable to generate answer after {self.max_steps} steps"
            self.log(f"‚ùå {error_msg}")
            self.save_log()
            return error_msg


# ------------------------
# Example usage
# ------------------------
if __name__ == "__main__":
    agent = ReActAgent()
    
    # Test with a question that might benefit from internal knowledge first
    question = "Employee Benefits"
    print("Testing with question:", question)
    answer = agent.run(question)